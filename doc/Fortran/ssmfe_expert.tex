\packagename{SSMFE\_EXPERT}
\version{1.0.0}
\versiondate{8 April 2015}

\newcommand{\solver}{ssmfe}
\newcommand{\Solver}{SSMFE\_EXPERT}

\newcommand{\engine}{{\tt SPRAL\_SSMFE\_CORE}}
\newcommand{\simple}{{\tt SPRAL\_SSMFE}}
\newcommand{\fullpackagename}{{\tt SPRAL\_\Solver}}

\newcommand{\report}{Technical Report RAL-TR-2010-19}

\newcommand{\Integer}{{\tt INTEGER}}
\newcommand{\Character}{{\tt CHARACTER}}
\newcommand{\Logical}{{\tt LOGICAL}}
\newcommand{\REALDP}{\texttt{REAL}}

\newcommand{\cV}{{\cal V}}
\newcommand{\cX}{{\cal X}}
\newcommand{\Span}{\,\mbox{\rm span}}

\newcommand{\itt}[1]{{\item {\tt #1}}}
\newcommand{\Ref}[1]{{\rm (\ref{#1})}}


\purpose{
\fullpackagename\ 
computes extreme (leftmost and/or rightmost)
eigenpairs $\{\lambda_i, x_i\}$ of the following eigenvalue problems:
%
\begin{itemize}
\item 
the standard eigenvalue problem
%
\begin{eqnarray}
\label{evp}
A x = \lambda x,
\end{eqnarray}
%
\item
the generalized eigenvalue problem
%
\begin{eqnarray}
\label{evp.g}
A x = \lambda B x,
\end{eqnarray}
%
\item
the buckling problem
%
\begin{eqnarray}
\label{evp.b}
B x = \lambda A x,
\end{eqnarray}
%
\end{itemize}
%
where 
$A$ and $B$ are {\bf real symmetric} (or {\bf Hermitian}) matrices
and $B$ is {\bf positive definite}.

\fullpackagename\ 
delegates 
a considerable part of the computation to the user,
which makes its solver procedures rather difficult to use.
The user is advised to consider first using
the package \simple, which
offers a simple interface
to \fullpackagename.
}

\title{Sparse Symmetric Matrix-Free Eigensolver, \\[5pt]
expert interface}
\author{
   Evgueni Ovtchinnikov (STFC Rutherford Appleton Laboratory)
}
\pkglang{Fortran}
\spralmaketitle
\thispagestyle{firststyle}

\section*{Major version history}
\begin{description}
\item[2014-11-20 Version 1.0.0] Initial release
\end{description}

%%%%%%%%%%%%%%%%%%%%%% installation %%%%%%%%%%%%%%%%%%%%%%

\section{Installation}
Please see the SPRAL install documentation. In particular note that:
\begin{itemize}
   \item A BLAS library is required.
   \item A LAPACK library is required.
\end{itemize}

\section{Usage overview}

\label{sec:summary}

The eigensolver subroutines
behind \fullpackagename\
implement a block iterative algorithm.
The block nature of this algorithm allows the user
to benefit from highly optimized linear algebra subroutines
and from the ubiquitous multicore architecture
of modern computers.
It also makes this algorithm more reliable
than Krylov-based algorithms employed e.g. by ARPACK
in the presence of clustered eigenvalues.
However, convergence of the iterations may be slow
if the density of the spectrum is high.

Thus, good performance 
(in terms of speed)
is contingent on the following two factors:
(i) the number of desired
eigenpairs must be substantial
(e.g. not less than the number of CPU cores),
and
(ii) the employment of a convergence acceleration technique.
The acceleration techniques that can be used 
are shift-and-invert and preconditioning.
The former requires
the direct solution of linear systems
with the matrix $A$ or its linear combination with $B$,
for which a sparse symmetric indefinite solver
(such as {\tt HSL\_MA97} or {\tt SPRAL\_SSIDS}) can be employed.
The latter applies to the case of positive definite $A$ and
requires a matrix or an operator\footnote{
That is, an algorithm producing a vector $v = T u$ for a given
vector $u$.
}
$T$, called {\em a preconditioner},
such that the vector
$v = T f$ is an approximation to the solution $u$
of the system $A u = f$.
%(see a simple example in Section~\ref{sec:ex.prec}).
This technique is more sophisticated
and is likely to be of interest only to experienced users.

Further information on the algorithm used by
\fullpackagename\ can be found in the
specification document for \engine\
and in \report.

%Compared to the package \simple, which is built upon 
\fullpackagename,
%the latter 
delegates 
a considerable part of the computation to the user.
The user's code stores all  vectors  of size equal to the problem size $n$.
\fullpackagename\
is not ``aware'' of $n$ or how these vectors are stored; 
all operations on these vectors are performed by the user.
The amount of computation performed by 
the solver subroutines of \fullpackagename\
and the memory they use are negligible. 
These features facilitate the use of these subroutines
for shared-memory, out-of-core and hybrid computation.
A simpler but less flexible interface to
\fullpackagename\
is offered by \simple.

\subsection{Calling sequences}

\label{sec:call}

Access to the package requires a {\tt USE} statement \\ \\
\indent\hspace{8mm}{\tt use \fullpackagename} \\

\medskip

\noindent The following procedures are available to the user:
%
\begin{description}
\vspace{-0.1cm}
\item (a) {\tt \solver\_standard()} 
(for computing leftmost eigenpairs of \Ref{evp}, 
optionally using preconditioning if $A$ is positive definite)
\item (b) {\tt \solver\_standard\_shift()} 
(for computing eigenpairs of \Ref{evp} near a given shift %value
using the shift-and-invert technique)
\item (c) {\tt \solver\_generalized()} 
(for computing leftmost eigenpairs of 
\Ref{evp.g}, optionally using preconditioning if $A$ is positive definite)
\item (d) {\tt \solver\_generalized\_shift()} 
(for computing eigenpairs of 
\Ref{evp.g} near a given shift %value
using the shift-and-invert technique)
\item (e) {\tt \solver\_buckling()} 
(for computing eigenpairs of 
\Ref{evp.b} near a given shift %value
using the shift-and-invert technique)
\item (f) {\tt \solver\_terminate()} 
%
\end{description}

The solver procedures (a)--(e)
must be called repeatedly using
a reverse communication interface.
The terminating procedure (f)
should be called once after the
final call to 
a solver procedure
to deallocate all arrays 
that have been allocated by %any of 
the solver procedure.

\if 0
Several problems can be solved simultaneously,
i.e. the package does not require the solution of
one problem to be finished before the solution of
the next starts, as long as for each problem a separate set
of arguments for the above subroutines is used.
However, if two or more problems of the same type
need to be solved, it is reasonable to solve them one
after another  to reduce  memory requirements.
\fi

\subsection{Package types} 

\Integer\ denotes default \Integer, and \REALDP\ denotes
\texttt{REAL(kind=kind(0d0))}. We use the term {\bf package type} to mean
\texttt{REAL(kind=kind(0d0))} for calls to the double precision real valued
interface, and to mean \texttt{COMPLEX(kind=kind(0d0))} for calls to the double
precision complex valued interface.

\subsection{Derived types}
\label{derived types}

For each problem, the user must employ the derived types defined by the
module to declare scalars of the types 
{\tt \solver\_rcid} (real version) or 
{\tt \solver\_rciz} (complex version), 
{\tt \solver\_expert\_keep},
{\tt \solver\_options} and 
{\tt \solver\_inform}.
The following pseudocode illustrates this.
\begin{verbatim}
      use SPRAL_SSMFE_EXPERT
      ...
      type (ssmfe_rcid       ) :: rcid
      type (ssmfe_expert_keep) :: keep
      type (ssmfe_options    ) :: options
      type (ssmfe_inform     ) :: inform
      ...
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%% argument lists %%%%%%%%%%%%%%%%%%%%%%%%

\section{Argument lists}

\subsection{Solver procedures}

{\bf
To compute %several 
the leftmost eigenpairs of \Ref{evp},
optionally using preconditioning,
the following call must be made repeatedly:
}

\medskip
{\tt call
\solver\_standard( rci, left, mep, lambda, m, rr, ind, keep, options, inform )
}

\medskip
\noindent
{\bf
To compute the eigenvalues of \Ref{evp} %around 
in the vicinity of a given value {\tt sigma}
and the corresponding eigenvectors using the shift-and-invert technique,
the following call must be made repeatedly:
}

\medskip
{\tt call
\solver\_standard\_shift \&

\hspace{8mm} 
( rci, sigma, left, right, mep, lambda, m, rr, ind, keep, options, inform )
}

\medskip
\noindent
{\bf
To compute the leftmost eigenpairs of \Ref{evp.g},
optionally using preconditioning,
the following call must be made repeatedly:
}

\medskip
{\tt call
\solver\_generalized( rci, left, mep, lambda, m, rr, ind, 
keep, options, inform )
}

\medskip
\noindent
{\bf
To compute the eigenvalues of \Ref{evp.g} %around 
in the vicinity of 
a given value {\tt sigma}
and the corresponding eigenvectors using the shift-and-invert technique,
the following call must be made repeatedly:
}

\medskip
{\tt call
\solver\_generalized\_shift \&

\hspace{8mm} 
( rci, sigma, left, right, mep, lambda, m, rr, ind, keep, options, inform )
}

\medskip
\noindent
{\bf
To compute the eigenvalues of \Ref{evp.b}
in the vicinity of a given value {\tt sigma}
and the corresponding eigenvectors 
the following call must be made repeatedly:
}

\medskip
{\tt call
\solver\_buckling \&

\hspace{8mm} 
( rci, sigma, left, right, mep, lambda, m, rr, ind, keep, options, inform )
}

\medskip
To use the solver procedures,
the user needs to maintain a workspace {\tt W} containing
{\tt kw + 1} blocks of {\tt m} vectors of size $n$.
A value {\tt kw = 7} is always sufficient. 
However, if {\tt options\%minAprod} $=$ {\tt .false.}
and either {\tt options\%minBprod} $=$ {\tt .false.} or 
the standard eigenvalue problem \Ref{evp} is solved,
then {\tt kw = 3} is sufficient; 
if 
{\tt options\%minAprod} $=$ {\tt .true.} and
{\tt options\%minBprod} $=$ {\tt .true.},
then {\tt kw} must be at least {\tt 7};
otherwise {\tt kw = 5} is sufficient.
Solver procedures
use indices {\tt 1} to {\tt m} 
to refer to vectors inside each block
and indices {\tt 0} to {\tt kw} 
to refer to particular blocks.
The first (zero-indexed) block holds the eigenvector approximations:
the user must fill this block with 
{\tt m} linearly independent vectors before the first call
to a solver procedure.

The number of desired eigenpairs may exceed {\tt m}:
whenever converged eigenpairs have been detected,
a solver procedure reports the indices of these eigenpairs
and they must be moved by the user
to a separate eigenvectors' storage {\tt X}.

When $B \ne I$,
it is expedient to 
have %another 
storage {\tt BX}
for the $B$-images of the converged eigenvectors,
i.e. {\tt BX = B*X}.

To simplify the description of the %advanced RCI's,
reverse communication interface,
below we assume that an array
{\tt W(n,m,0:kw)} of package type
is used as a workspace,
and that arrays {\tt X(n, mep)} and {\tt BX(n, mep)} of package type
are used for storing the computed eigenvectors
and their $B$-images.
The transpose (real or complex, depending on the package type)
of a matrix {\tt H} 
is denoted by {\tt H}$^\prime$.

\medskip
The meaning of the arguments of the solver procedures is as follows.

\begin{description}
%
\itt{rci} is is an \intentinout\ scalar  of type
{\tt \solver\_rcid} in the real version and
{\tt \solver\_rciz} in the complex version.
Before the first call, {\tt rci\%job} must be set to {\tt 0}.
No other values may be assigned to {\tt rci\%job} by the user.
After each call,
the value of {\tt rci\%job} must be inspected by the user's code
and the appropriate action taken: 
%(see below for details).
%The following values of {\tt rci\%job}
%are common to all solver procedures 
%and require the same action: 
%
\begin{description}
%
\itt{-3}: fatal error return, the computation must be terminated;
%
\itt{-2}: 
not all desired eigenpairs converged to required accuracy,
see Section~\ref{sec:err}; 
%
\itt{-1}: the computation is complete and successful.
%
\itt{~1}:
({\tt \solver}\ and {\tt \solver\_gen}\ only)
the user must compute $V = A U$, where

\hspace{8mm}
$U=$ {\tt W(:, ix:jx, rci\%kx)}, 
~with~ {\tt ix} $=$ {\tt rci\%jx} 
~and~
{\tt jx} $=$ {\tt ix + rci\%nx - 1},

\hspace{8mm}
$V=$ {\tt W(:, iy:jy, rci\%ky)},
~with~ {\tt iy} $=$ {\tt rci\%jy} 
~and~
{\tt jy} $=$ {\tt iy + rci\%nx - 1}.
%
\itt{~2}:
({\tt \solver\_standard} and {\tt \solver\_generalized} only)
the user must
compute $V = T U$ if preconditioning is used
or copy $U$ to $V$ otherwise,
where $U$ and $V$ are as for {\tt rci\%job = 1}.
%
\itt{~3}:
({\tt \solver\_generalized}, {\tt \solver\_generalized\_shift} 
and {\tt \solver\_buckling} only)
the user must compute $V = B U$ 
where $U$ and $V$ are as for {\tt rci\%job = 1}.
%
\itt{~5}: the user must save the converged eigenvectors
to the eigenvector storage {\tt X}
and, optionally, 
for problems \Ref{evp.g} and \Ref{evp.b},
save their $B$-images.
The converged eigenvectors are columns of the ${\tt n}\times {\tt m}$ matrix
{\tt W(:,:,rci\%kx)} and their $B$-images are respective columns of
{\tt W(:,:,rci\%ky)}
that are identified by
{\tt rci\%i, rci\%jx} and {\tt rci\%nx}
as follows:
if {\tt rci\%i > 0}, then the column numbers
run from {\tt rci\%jx} to {\tt rci\%jx + rci\%nx - 1},
and if {\tt rci\%i < 0}, then they run
from {\tt rci\%jx - rci\%nx + 1} to {\tt rci\%jx}.
%
\itt{~9}:
({\tt \solver\_standard\_shift}, {\tt \solver\_generalized\_shift}
and {\tt \solver\_buckling} only)
the user must compute $V = A_\sigma^{-1} U$, where
$A_\sigma = A - \sigma I$
and $I$ is $n\times n$ identity,
for problem \Ref{evp}, 
$A_\sigma = A - \sigma B$ for problem \Ref{evp.g},
and 
$A_\sigma = B - \sigma A$ for problem \Ref{evp.b}.
%
\itt{11}:
if {\tt rci\%i = 0}, then
the user  must perform a copy $V \leftarrow U$, 
where $U$ and $V$ are as for {\tt rci\%job = 1},
otherwise the columns of {\tt W(:,:,rci\%kx)}
and {\tt W(:,:,rci\%ky)}
(if {\tt rci\%kx} $\not=$ {\tt rci\%ky}) 
must be reordered using
the index array {\tt ind} so that %, i.e.
the column {\tt ind(j)} becomes column {\tt j}
for {\tt j = 1, \ldots, rci\%nx}.
%
\itt{12}:
for each
{\tt i = 0, 1,..., rci\%nx - 1}, 
the user must compute the dot product of
the columns 

\hspace{8mm}
{\tt W(:, rci\%jx + i, rci\%kx)} 

and

\hspace{8mm}
{\tt W(:, rci\%jy + i, rci\%ky)}

and place it in 

\hspace{8mm}
{\tt rr(rci\%i + i, rci\%j + i, rci\%k)}.
%
\itt{13}: 
if {\tt rci\%kx} $=$ {\tt rci\%ky}, then
for each
{\tt i = 0, 1,..., rci\%nx - 1}, 
the user must perform the scaling

\hspace{8mm}
{\tt W(:, rci\%jx + i, rci\%kx) = W(:, rci\%jx + i, rci\%kx)$/s_i$},

where $s_i$ is the 2-norm of the column 
{\tt W(:, rci\%jx + i, rci\%kx)},
otherwise the user must perform the scalings

\hspace{8mm}
{\tt W(:, rci\%jx + i, rci\%kx) = W(:, rci\%jx + i, rci\%kx)$/s_i$}

\hspace{8mm}
{\tt W(:, rci\%jy + i, rci\%ky) = W(:, rci\%jy + i, rci\%ky)$/s_i$},

where $s_i$ is the square root of the dot product of 
the columns 
{\tt W(:, rci\%jx + i, rci\%kx)} and
{\tt W(:, rci\%jy + i, rci\%ky)}.
No scaling is to be applied to zero columns.
%
\item
{\tt 14}: 
for each {\tt i = 0, 1,..., rci\%nx - 1}, 
the user must perform axpy-updates:

\hspace{8mm}
{\tt 
W(:, rci\%jy + i, rci\%ky) = 
W(:, rci\%jy + i, rci\%ky) + 
}

\hspace{12mm}
{\tt
rr(rci\%i + i, rci\%j + i, rci\%k) * W(:, rci\%jx + i, rci\%kx)}.
%
\item
{\tt 15}: the user must perform the matrix multiplication:

\hspace{8mm}
{\tt rr(rci\%i $:$ rci\%i + rci\%nx - 1, rci\%j $:$ 
rci\%j + rci\%ny-1, rci\%k) =}

\hspace{12mm}
{\tt rci\%alpha *
W(:, rci\%jx $:$ rci\%jx + rci\%nx - 1, rci\%kx)' *}

\hspace{16mm}
{\tt W(:, rci\%jy $:$ rci\%jy + rci\%ny - 1, rci\%ky) +}

\hspace{12mm}
{\tt 
rci\%beta *
rr(rci\%i $:$ rci\%i + rci\%nx - 1, rci\%j $:$ rci\%j + rci\%ny - 1, rci\%k)}.

%where {\tt W'} denotes the transpose of {\tt W};
%
\item
{\tt 16}: the user must perform the matrix multiplication:

\hspace{8mm}
{\tt W(:, rci\%jy $:$ rci\%jy + rci\%ny - 1, rci\%ky) =}

\hspace{12mm}
{\tt rci\%alpha *
W(:, rci\%jx $:$ rci\%jx + rci\%nx - 1, rci\%kx) *}

\hspace{16mm}
{\tt rr(rci\%i $:$ rci\%i + rci\%nx - 1, 
rci\%j $:$ rci\%j + rci\%ny - 1, rci\%k) + }

\hspace{12mm}
{\tt rci\%beta * W(:, rci\%jy $:$ rci\%jy + rci\%ny - 1, rci\%ky)}.
%
\item
{\tt 17}: the user must perform the multiplication:


\hspace{8mm}
{\tt W(:, rci\%jx $:$ rci\%jx + rci\%ny - 1, rci\%kx) =}

\hspace{12mm}
{\tt W(:, rci\%jx $:$ rci\%jx + rci\%nx - 1, rci\%kx) *}

\hspace{16mm}
{\tt rr(rci\%i $:$ rci\%i + rci\%nx - 1, 
rci\%j $:$ rci\%j + rci\%ny - 1, rci\%k)}.

{\tt W(:, rci\%jy $:$ rci\%jy + rci\%ny - 1, rci\%ky)}
can be used as a workspace.
%
\item
{\tt 21}: 
the user must $B$-orthogonalize
the columns of {\tt W} specified by
{\tt rci\%nx}, {\tt rci\%jx} and {\tt rci\%kx} 
to all vectors stored in {\tt X}
by solving the system

\hspace{8mm}
{\tt (X' * BX) Q = X' * W(:, rci\%jy $:$ rci\%jy + rci\%nx - 1, rci\%ky)}

for {\tt Q} and updating

\hspace{8mm}
{\tt W(:, rci\%jx $:$ rci\%jx + rci\%nx - 1, rci\%kx) =}

\hspace{12mm}
{\tt W(:, rci\%jx $:$ rci\%jx + rci\%nx - 1, rci\%kx) - X * Q}.

For problems \Ref{evp.g} and \Ref{evp.b},
the respective columns of {\tt W(:,:,rci\%ky)},
which store $B$-images of the respective columns of {\tt W(:,:,rci\%kx)},
must be updated
accordingly, either by applying {\tt B} to these vectors
or using the columns of {\tt BX}, i.e.

\hspace{8mm}
{\tt W(:, rci\%jy $:$ rci\%jy + rci\%nx - 1, rci\%ky) =}

\hspace{12mm}
{\tt W(:, rci\%jy $:$ rci\%jy + rci\%nx - 1, rci\%ky) - BX * Q};
%
\item
{\tt 22}: 
the user must solve the system

\hspace{8mm}
{\tt (X' * BX) Q = 
X' * W(:, rci\%jx $:$ rci\%jx + rci\%nx - 1, rci\%kx)}

for {\tt Q}  and perform the update

\hspace{8mm}
{\tt W(:, rci\%jx $:$ rci\%jx + rci\%nx - 1, rci\%kx) =}

\hspace{12mm}
{\tt W(:, rci\%jx $:$ rci\%jx + rci\%nx - 1, rci\%kx) - BX * Q},

where {\tt X} and {\tt BX} are same as in 
the case {\tt rci\%job = 21}
(in the case of problem \Ref{evp},
{\tt rci\%job = 21} and {\tt 22} 
require exactly the same computation).
%
\item
{\tt 999}: 
If {\tt rci\%k > 0}, then
a restart, normally with a larger block size {\tt m},
is suggested with the aim of achieving better convergence.
If the suggestion is accepted, the user must compute
the new block size as {\tt m = rci\%nx + k + l},
where {\tt k $\ge$ rci\%i} and {\tt l $\ge$ rci\%j},
reallocate the workspace array {\tt W}
if the new block size is different from the old one,
and set {\tt rci\%i = 0} and {\tt rci\%j = 0}.
If the restart is not acceptable
(e.g. the new block size exceeds a certain limit set by the user), 
then nothing needs to be done.
If {\tt rci\%k == 0}, then
the restart with the same block size {\tt m} is required.
In both restart cases,
the first block {\tt W(:,:,0)} of the new
workspace should retain the vectors 
{\tt W(:,i:j,0)},
where {\tt i = rci\%jx} and {\tt j = i + rci\%nx - 1},
from the old workspace.
The remaining {\tt m - rci\%nx} columns of {\tt W(:,:,0)}
must be filled
with arbitrary vectors that are linearly independent from 
the converged eigenvectors and such that
the entire set of the columns of {\tt W(:,:,0)}
is linearly independent.
%
\end{description}
%
{\bf Restriction:} 
{\tt rci\%job = 0}, 
{\tt rci\%i = 0} and
{\tt rci\%j = 0} 
are the only %values that can be assigned 
assignments to the components of {\tt rci}
that can be done
by the user.
The first one can only be done before the first call.
The other two can only be done if
{\tt rci\%job = 999} and {\tt rci\%k > 0}.
%
\itt{sigma} 
({\tt \solver\_standard\_shift}, {\tt \solver\_generalized\_shift}
and {\tt \solver\_buckling} only)
is an \intentin\ scalar of type \REALDP\
that holds the shift, 
a value around which the desired eigenvalues %of interest 
are situated.
%
\itt{left} is an \intentin\ scalar of type default \Integer\ 
that holds the number of desired eigenvalues to the left of {\tt sigma}.
{\bf Restriction:} {\tt $0 < $ left + right $\le$ min(mep, n/2)},
where {\tt right} is zero for {\tt\solver\_standard}\ and 
{\tt\solver\_generalized}.
%
\itt{right} 
({\tt \solver\_standard\_shift}, {\tt \solver\_generalized\_shift}
and {\tt \solver\_buckling} only)
is an \intentin\ scalar of type default \Integer\ 
that holds the number of desired eigenvalues to the right of {\tt sigma}.
{\bf Restriction:} {\tt $0 < $ left + right $\le$ min(mep, n/2)}.
%
\itt{mep} is an \intentin\ scalar of type default \Integer\ 
that holds the size of the array {\tt lambda}.
See Section~\ref{sec:method} for guidance on
setting {\tt mep}.
{\bf Restriction:} 
{\tt mep} is not less than the number of desired eigenpairs.
%
\itt{lambda(:)} is an \intentinout\
array of type \REALDP\
and size {\tt mep} that is
used to store the computed eigenvalues.
After a successful completion of the computation
it contains eigenvalues in ascending order.
This array must not be changed by the user.
%
\itt{m} is an \intentin\ scalar of type {\tt INTEGER} that
holds the block size of the user's workspace {\tt W}. 
{\bf Restriction:} \\{\tt 2 $\le$ m $<$ n}.
%
\itt{rr(:,:,:)} is an \intentinout\
work array of package type,
and dimensions {\tt 2*m}, {\tt 2*m} and {\tt 3}.
It can only be changed by the user when
instructed to do so by 
%the reverse communication flag 
{\tt rci\%job}.
%
\itt{ind(:)} is an \intentinout\ 
array of default integer type, and size at least {\tt m}. 
It must not be changed by the user.
It is used for reordering the columns of some blocks of {\tt W}.
%
\itt{keep} is an \intentinout\ scalar of type 
{\tt \solver\_expert\_keep} 
that holds private data. 
%
\itt{options} is an \intentin\  scalar  of type {\tt \solver\_options}.
Its components offer the user a range of options,
see Section~\ref{sec:options}.
It must not be changed by the user between calls.
%inside the reverse communication loop.
%
\itt{inform} is an \intentinout\ scalar of type 
{\tt \solver\_inform}. Its components provide information about the execution
of the subroutine, see Section~\ref{sec:inform}.
It must not be changed by the user.
%
\end{description}

\subsection{Terminating procedure}

{\bf
At the end of the computation, the memory 
allocated by the solver procedures
should be released
by making the following subroutine call:
}

\medskip

\hspace{8mm} {\tt \solver\_free( keep, inform )}

\begin{description}
%
\itt{keep} is an \intentinout\ scalar of type 
{\tt \solver\_expert\_keep}, optional. 
On exit, its components that are allocatable arrays will have been deallocated. 
%
\itt{inform} is an \intentinout\ scalar of type {\tt \solver\_inform}, 
optional.
On exit, its components that are allocatable arrays will have been deallocated. 
%
\end{description}

\section{Derived types}

\subsection{Derived data type for options}

\label{sec:options}

The derived data type
{\tt \solver\_options}
has the following components.

\bigskip
\noindent
{\bf Convergence control options}

\begin{description}
%
\itt{abs\_tol\_lambda} is a scalar of type %default \Double\ 
\REALDP\ that
holds an absolute tolerance used when testing the estimated eigenvalue 
error, see Section~\ref{sec:method}. 
The default value is 0. %{\tt abs\_tol = 0}.
Negative values are treated as the default.
%
\itt{abs\_tol\_residual} is a scalar of type %default \Double\ 
\REALDP\ that
holds an absolute tolerance used when testing the residual, 
see Section~\ref{sec:method}.
The default value is 0.
Negative values are treated as the default.
%
\itt{max\_iterations} is a scalar of type default \Integer\ that
contains the maximum number of iterations to be performed.
The default value is 100. %{\tt max\_it = 100}.
{\bf Restriction:} {\tt max\_it $\ge$ 0}.
%
\itt{rel\_tol\_lambda} is a scalar of type %default \Double\ 
\REALDP\ that
holds a relative tolerance used when testing the estimated eigenvalue 
error, see Section~\ref{sec:method}. 
The default value is 0. %{\tt 10*epsilon(lambda)}.
Negative values are treated as the default.
%
\itt{rel\_tol\_residual} is a scalar of type %default \Double\ 
\REALDP\ that
holds a relative tolerance used when testing the residual,
see Section~\ref{sec:method}. 
If both {\tt abs\_tol\_residual} and {\tt rel\_tol\_residual}
are set to 0, then the residual norms are not taken
into consideration by the convergence test,
see Section~\ref{sec:method}.
The default value is 0.
Negative values are treated as the default.
%
\itt{tol\_x} is a scalar of type \REALDP\ %default \Double\ 
that holds a tolerance used when testing the estimated 
eigenvector error, see Section~\ref{sec:method}. 
If {\tt tol\_x} is set to zero, the eigenvector error is not estimated.
If a negative value is assigned, the tolerance is set to
{\tt 10*epsilon(lambda)}.
The default value is -1.0.
%
\end{description}

\medskip
\noindent
{\bf Printing options}

\begin{description}
%
\itt{print\_level} is a scalar of type default \Integer\ that
determines the amount of printing.
Possible values are:
%
\begin{tabular}{rcl}
$<0$ &:& no printing;\\
$0$ &:& error and warning messages only;\\
$1$ &:& the type (standard or generalized) and the size of the problem,
the number of eigenpairs \\ 
& & requested, the error tolerances and
the size of the subspace are printed before the iterations start;\\
$2$ &:& as $1$ but, for each eigenpair tested for convergence
(see Section~\ref{sec:method}), the iteration number,
the index of \\
& & 
the eigenpair, the
eigenvalue, whether it has converged, the residual norm, and the error
estimates \\
& & are printed;\\
$>2$ &:& as $1$ but with all eigenvalues, whether converged, residual norms
and eigenvalue/eigenvector\\ 
& & error estimates printed on each iteration. 
\end{tabular}

\noindent
The default value is 0. %{\tt verbosity = 0}.
Note that for eigenpairs that are far from convergence,
`rough' error estimates are printed
(the estimates that are actually used by the stopping criteria,
see Section~\ref{sec:method}, only become available on the last few
iterations).
%
\itt{unit\_error} is a scalar of type default \Integer\ that
holds the unit number for error messages.
Printing is suppressed if {\tt unit\_error < 0}.
The default value is 6. 
%
\itt{unit\_diagnostic} is a scalar of type default \Integer\ that
holds the unit number for messages  monitoring the convergence.
Printing is suppressed if {\tt unit\_diagnostics < 0}.
The default value is 6. 
%
\itt{unit\_warning} is a scalar of type default \Integer\ that
holds the unit number for warning messages.
Printing is suppressed if {\tt unit\_warning < 0}.
The default value is 6. 
%
\end{description}

\medskip
\noindent
{\bf Advanced options}

\begin{description}
%
\itt{err\_est} 
is a scalar of type default \Integer\ that
defines which error estimation scheme 
for eigenvalues and eigenvectors
is to be used by the stopping criterion.
Two schemes are implemented.
If {\tt err\_est = 1}, residual error bounds are used,
namely,
a modified Davis-Kahan estimate for the eigenvector error
and
the Lehmann bounds for the eigenvalue error.
(see Section~\ref{sec:err.est}).
If {\tt err\_est = 2}, 
then the eigenvector and eigenvalue errors
are estimated by analyzing the convergence curve
for the eigenvalues (see Section~\ref{sec:err.est}).
The default is {\tt err\_est = 2}.
{\bf Restriction:} {\tt err\_est = 1 {\rm or} 2}.
%
\itt{extra\_left}
is a scalar of type default \Integer\ that
holds the number of extra approximate eigenvectors
corresponding to leftmost eigenvalues
that are of no interest to the user
and are iterated solely to enhance convergence.
The default is {\tt extra\_left = 0}.
{\bf Restriction:} {\tt extra\_left $\ge$ 0}.
%
\itt{extra\_right}
is a scalar of type default \Integer\ that
holds the number of extra approximate eigenvectors
corresponding to rightmost eigenvalues
that are of no interest to the user
and are iterated solely to enhance convergence.
The default is {\tt extra\_right = 0}.
{\bf Restriction:} {\tt extra\_right $\ge$ 0}.
%
\itt{left\_gap}
is a scalar of type \REALDP\
that is only used when
{\tt left} is non-zero, and
specifies the minimal acceptable distance
between the last computed left eigenvalue
and the rest of the spectrum.
For {\tt \solver} and {\tt \solver\_gen},
the last computed left eigenvalue
is the rightmost of the computed ones,
and for the other procedures
it is the leftmost.
If set to a negative value $\delta$,
the minimal distance is taken as
$|\delta|$ times the average distance between the computed eigenvalues.
Note that for this option to have any effect,
the value of {\tt mep} must be larger than
{\tt left + right}: see Section~\ref{sec:method}
for further explanation.
The default value is 0.
%
\itt{max\_left}
is a scalar of type default \Integer\ that
holds the number of eigenvalues to the left from $\sigma$,
or a negative value, if this number is not known
(cf. \S\ref{sec:si}).
The default is {\tt max\_left = -1}.
%{\bf Restriction:} {\tt extra\_left $\ge$ 0}.
%
\itt{max\_right}
is a scalar of type default \Integer\ that
holds the number of eigenvalues to the right from $\sigma$,
or a negative value, if this number is not known.
(cf. \S\ref{sec:si}).
The default is {\tt max\_right = -1}.
%
\itt{minAprod} is a scalar of type default \Logical\ that
determines whether the number of multiplications by $A$ 
is to be reduced at the expense of memory. 
If ${\tt minAprod = .false.}$, 
on each iteration three returns to the user
with {\tt rci\%job = 1} are
made for multiplications of {\tt rci\%nx} vectors by $A$.
Otherwise,  only one such return is made at each iteration but 
the number {\tt kw} of blocks in the user's work array {\tt W} 
must be increased by {\tt 2}.
The default is {\tt minAprod = .true.}.
{\bf Restriction:} {\tt minAprod = .true.} %if {\tt problem < 0}.
for {\tt\solver\_shift}, {\tt\solver\_gen\_shift}
and  {\tt\solver\_buckling}.
%
\itt{minBprod} is a scalar of type default \Logical\ that
determines whether the number of multiplications by $B$ 
is to be reduced at the expense of memory. 
If ${\tt minBprod = .false.}$, 
on each iteration at least three returns to the user
with {\tt rci\%job = 3} are
made for multiplications of {\tt rci\%nx} vectors by $B$.
Otherwise,  only one such return is made at each iteration but 
the number {\tt kw} of blocks in the user's work array {\tt W} 
must be increased by {\tt 2}.
The default is {\tt minBprod = .true.}.
{\bf Restriction:} {\tt minBprod = .true.} %if {\tt problem < 0}.
for {\tt\solver\_shift}, {\tt\solver\_gen\_shift}
and  {\tt\solver\_buckling}.
%
\itt{right\_gap}
is a scalar of type \REALDP\ %default \Double\ 
that is only used by 
{\tt \solver\_shift}, {\tt \solver\_gen\_shift}
and {\tt \solver\_buckling}
with a non-zero {\tt right}, and
has the same meaning as {\tt options\%left\_gap}
but for the rightmost computed eigenvalue.
The default value is 0.
%
\itt{user\_x} is a scalar of type default \Integer. 
If {\tt user\_x > 0} then the first {\tt user\_x} columns
of {\tt x(:,:)} will be used as initial guesses for eigenvectors.
Hence, if the user has good approximations
to some of the required eigenvectors, the computation time
may be reduced by putting these approximations
into the first {\tt user\_x} columns of {\tt x(:,:)}.
The default value is 0, 
i.e. the columns of {\tt x(:,:)} are overwritten by the solver.
{\bf Restriction:} {0 $\le$ \tt user\_x $\le$ m},
the first {\tt user\_x} columns in {\tt x(:,:)}
must be linearly independent.
%
\end{description}

\subsection{The derived data type for holding information}

\label{sec:inform}

The derived data type {\tt ssmfe\_inform} is used
to hold information from the execution of
the solver procedures.
The components are:

\begin{description}
%
\itt{converged} is a rank-1 allocatable array of type 
default \Integer\ that is allocated to have size {\tt mep} 
on a call with {\tt rci\%job = 0} or {\tt 999}.
If, on some iteration {\tt i}, an eigenpair ({\tt lambda(j), X(j)})
has been accepted as converged,
then {\tt converged(j) = i}; if the convergence stagnated
then {\tt converged(j) = -i}; otherwise {\tt converged(j) = 0}.
%
\itt{err\_lambda} is a rank-1 allocatable array of type 
{\tt REAL} that is allocated to have size {\tt mep} 
on a call with {\tt rci\%job = 0} or {\tt 999}.
{\tt err\_lmd(i)} contains 
the estimated eigenvalue error
for the approximate eigenvalue {\tt lambda(i)}
if {\tt info\%converged(i)} is non-zero,
and {\tt -1.0} otherwise.
%
\itt{err\_x} is a rank-1 allocatable array of type {\tt REAL}.
This array is allocated to have size {\tt mep} 
on a call with {\tt rci\%job = 0} or {\tt 999},
and is used for storing the eigenvector errors
in the same way as {\tt err\_lmd} is used
for storing the eigenvalue errors.
%
\itt{flag} is a scalar of type default \Integer\ 
that is used as an error flag.
If a call is successful, {\tt flag} has value {\tt 0}.
A nonzero value of {\tt flag} indicates an error or a warning
(see Section~\ref{sec:err.solve}).
%
\itt{iteration} is a scalar of type default \Integer\
that holds the number of iterations 
since the previous {\tt rci\%job = 0} or {\tt rci\%job = 999} call.
%
\itt{left}
is a scalar of type default \Integer\
that holds the number of converged eigenvalues on the left,
i.e. the total number of converged eigenpairs of \Ref{evp}
or the number of the converged eigenvalues 
of \Ref{evp.g} or \Ref{evp.b}
to the left of {\tt sigma}. 
%
\itt{next\_left}
is a scalar of type default {\tt REAL} that holds
the non-converged eigenvalue next to the last converged on the left
(cf. {\tt options\%left\_gap}).
%
\itt{next\_right}
is a scalar of type default {\tt REAL} that holds
the non-converged eigenvalue next to the last converged on the right
(cf. {\tt options\%right\_gap}).
%
\itt{non\_converged}
is a scalar of type default \Integer\ 
that holds the number of non-converged eigenpairs
%additional information about errors and warnings
(see Section~\ref{sec:err.solve}).
%
\itt{residual\_norms} is a rank-1 allocatable array of type
default {\tt REAL} that is allocated to have size {\tt mep} 
on a call with {\tt rci\%job = 0} or {\tt 999}.
On returns with 
{\tt rci\%job = 5},
{\tt residual\_norms(i)} 
contains the Euclidean norm of the residual
for {\tt lambda(i), X(i)}.
%
\itt{right}
is a scalar of type default \Integer\ 
that holds the number of converged eigenvalues 
of \Ref{evp.g} or \Ref{evp.b}
to the right of {\tt sigma}.
%
\itt{stat}
is a scalar of type default \Integer\ 
that holds the allocation status
%additional information about errors and warnings
(see Section~\ref{sec:err.solve}).
%
\end{description}

\subsection{Error codes}

\label{sec:err}

A successful return from
a solver procedure
is indicated 
by {\tt inform\%flag$=$0}.
A negative value indicates an error, a positive value indicates a warning.
%{\tt info\%data} provides further information
%about some errors and warnings. 

\label{sec:err.solve}

Possible negative values of {\tt inform\%flag}
are as follows:
%
\begin{description}
%
\item{~~-1}
\hskip 9pt
Incorrect value of {\tt rci\%job}.
%
\item{~~-2}
\hskip 9pt
Block size {\tt m} is out-of-range.
%
\item{~~-3}
\hskip 9pt
Incorrect value of 
{\tt options\%err\_est}. %is out-of-range.
%
\item{~~-4}
\hskip 9pt
Incorrect value of 
{\tt options\%minAprod} or {\tt options\%minBprod}.
%
\item{~~-5}
\hskip 9pt
Incorrect value of 
{\tt options\%extra\_left} or
{\tt options\%extra\_right}.
%
\item{~~-6}
\hskip 9pt
Incorrect value of 
{\tt options\%min\_gap}. %is out-of-range.
%
\item{~-11}
\hskip 7pt
Incorrect value of 
{\tt left}. % is out-of-range.
%
\item{~-12}
\hskip 7pt
Incorrect value of 
{\tt right}. %is out-of-range.
%
\item{~-13}
\hskip 7pt
{\tt mep} is less than 
the number of desired eigenpairs.
%{\tt left} + {\tt right}.
%
\item{-100}
\hskip 4pt
Not enough memory;
{\tt inform\%stat} contains the value of the Fortran {\tt stat} parameter.
%
\item{-200}
\hskip 4pt
$B$ is not positive definite or initial eigenvectors are linearly dependent.
%
\end{description}

Possible positive values  are: 
%
\begin{description}
\item{1}
\hskip 9pt
The iterations have been terminated because no further improvement
in accuracy is possible (this may happen if the preconditioner is
not positive definite, or if the components of the residual vectors
are so small that the round-off
errors make them essentially random).
The value of {\tt inform\%non\_converged} is set to the number
of non-converged eigenpairs.
\item{2}
\hskip 9pt
The maximal number of iterations has been exceeded.
The value of {\tt inform\%non\_converged} is set to the number
of non-converged eigenpairs.
\item{3}
\hskip 9pt
Out of storage space for the converged eigenpairs.
The value of {\tt inform\%non\_converged} is set to the number
of non-converged eigenpairs.
%
\end{description}

\section{Method}
\label{sec:method}

\subsection{The algorithm}

The solver procedures of
\fullpackagename\ %is built upon 
are interfaces to solver procedures of
{\tt \engine}, which 
implement a block iterative algorithm
based on the Jacobi-conjugate preconditioned gradients %(JCPG) 
method \cite{jcpg1,jcpg2}.
%This algorithm simultaneously computes $m < n$ approximate eigenpairs,
%where the block size $m$ exceeds the number $n_e$ of desired eigenpairs
%for the sake of better convergence, namely,
%$m = n_e + \min(10, 0.1 n_e)$.
Further information on the algorithm used by
\fullpackagename\ can be found in the
specification document for \engine\
and in \report.

\subsection{Stopping criteria}

An approximate eigenpair 
$\{x,\lambda\}$ is considered to have converged
if %all of 
the following three conditions are all satisfied:
%
\begin{enumerate}
%
\item
if {\tt options\%abs\_tol\_lambda} and 
{\tt options\%rel\_tol\_lambda}
are not both equal to zero, then
the estimated error in the approximate eigenvalue
must be less than or equal to

{\tt max(options\%abs\_tol\_lambda, 
$\delta$*options\%rel\_tol\_lambda)},

where $\delta$ is the estimated average distance
between eigenvalues.
\item
if {\tt options\%tol\_x} is not zero, then
the estimated sine of the angle between
the approximate eigenvector and the invariant subspace
corresponding to the eigenvalue 
approximated by $\lambda$
must be less than or equal to {\tt options\%tol\_x}.
\item
if {\tt options\%abs\_tol\_residual} and 
{\tt options\%rel\_tol\_residual}
are not both equal to zero, then
the Euclidean norm of the residual,
$\|A x - \lambda B x\|_2$,
must be less than or equal to

{\tt max(options\%abs\_tol\_residual, 
options\%rel\_tol\_residual*$\|\lambda B x\|_2$)}.
%
\end{enumerate}
%
The extra eigenpairs are not checked for convergence,
as their role is purely auxiliary.

\subsection{Improving eigenvector accuracy}

If the gap %distance 
between the last computed eigenvalue 
and the rest of the spectrum is small,
then the accuracy of the corresponding eigenvector may be very low.
To prevent this from happening,
the user should set the eigenpairs storage size {\tt mep}
to a value that is larger than the number of desired eigenpairs,
and set the options 
{\tt options\%left\_gap}
and
{\tt options\%right\_gap}
to non-zero values $\delta_l$ and $\delta_r$.
These values
determine the size of the minimal acceptable gaps
between the computed eigenvalues and the rest of the spectrum,
$\delta_l$ referring to either leftmost eigenvalues
(for {\tt \solver\_standard} and {\tt \solver\_generalized} only)
or those to the left of the shift {\tt sigma},
and $\delta_r$
to those to the right of the shift {\tt sigma}.
Positive values of $\delta_l$ and $\delta_r$
set the gap explicitly,
and negative values
require the gap to be not less than their absolute value times
the average distance between the computed eigenvalues.
A recommended value of $\delta_l$ and $\delta_r$ is $-0.1$.
The value of {\tt mep} %virtually does not affect 
has little effect on
the speed of computation,
hence it might be set to any reasonably large value.
The larger the value of {\tt mep}, 
the larger the size of an eigenvalue cluster
for which accurate eigenvectors can be computed, notably:
to safeguard against clusters of size up to $k$,
it is sufficient to set {\tt mep} to the number of desired eigenpairs
plus $k - 1$.

\subsection{The use of shifted matrix factorization}
\label{sec:si}

When using the solver procedures that employ the shift-and-invert technique,
it is very important to ensure that the numbers of desired eigenvalues
each side of the shift do not exceed the actual numbers of these eigenvalues,
as the eigenpairs `approximating' non-existing eigenpairs of the problem
will not converge.
It is therefore strongly recommended that the user employs 
a linear system solver that performs
the LDLT
factorization of %for solving 
the shifted system,
e.g. {\tt HSL\_MA97} or {\tt SPRAL\_SSIDS}.
The LDLT factorization of the matrix
$A - \sigma B$ consists in finding a unit lower triangular
matrix $L$, a block-diagonal matrix $D$
with $1\times 1$ and $2\times 2$ blocks on the main diagonal
and a permutation matrix $P$
such that $P^T(A - \sigma B)P = L D L^T$.
By inertia theorem,
the number of eigenvalues to the left and right from 
the shift $\sigma$
is equal to the number of negative and positive eigenvalues of $D$,
which allows quick computation of the eigenvalue numbers
each side of the shift. %(see the example in Section~\ref{sec:ex2}).

\subsection{Error estimation}

\label{sec:err.est}

\subsubsection{Standard problem}

If {\tt options\%err\_est} {\tt =} {\tt 1}, 
the error estimates for the eigenvalues are based on 
the eigenvalues of a matrix of the form
%
\begin{eqnarray}
\label{L.mx}
\hat A = %\Diag\{\lambda_j^i\}_{j=1}^{k-1} 
\tilde\Lambda_k - S_k^T S_k,
\end{eqnarray}
%
where $\tilde\Lambda_k$ is a diagonal matrix with
the $k-1$ leftmost Ritz values $\tilde\lambda_j$
on the diagonal,
and the columns of $S_k$ are the respective
residual vectors $r_j = A \tilde x_j - \tilde\lambda_j \tilde x_j$
divided by $\sqrt{\lambda_k - \tilde\lambda_j}$.
If $k$ is such that
$\tilde\lambda_{k-1} < \lambda_k$,
then the eigenvalues of $\hat A$ are
the left-hand side bounds for
eigenvalues $\lambda_i$,
%The minimax principle for eigenvalues implies that
%$\lambda_j \le \tilde\lambda_j$, 
and thus
the difference $\tilde\lambda_j - \hat\lambda_j$ estimates
the eigenvalue error $\tilde\lambda_j - \lambda_j$.
The unknown  $\lambda_k$ is replaced by $\tilde\lambda_k$,
and select the maximal $k \le m$ for which
the distance between $\tilde\lambda_{k-1}$ and $\tilde\lambda_k$
exceeds the sum of the absolute error tolerance for eigenvalues
and the Frobenius norm of the matrix formed by the residuals
$r_j, j = 1, \ldots, k-1$.
If  $\tilde\lambda_j - \hat\lambda_j$
is close to the machine accuracy, it may be too polluted
by round-off errors to rely upon.
In such case, we use instead
%
\begin{eqnarray}
\label{aL}
\tilde\lambda_j - \lambda_j \le \delta_j \approx
\frac{\|r_j\|^2}{\tilde\lambda_k - \lambda_j}.
\end{eqnarray}

The eigenvector errors are estimated based on 
the Davis-Kahan inequality:
%
\begin{eqnarray}
\label{DK}
\min_{x \in \cX_{k-1}}
\sin\{\tilde x_j; x\} \le
\frac{\|r_j\|}{\lambda_k - \tilde\lambda_j} \approx
\frac{\|r_j\|}{\tilde\lambda_k - \tilde\lambda_j},
\end{eqnarray}
%
where $\cX_{k-1}$ is the invariant subspace 
corresponding to $k-1$ leftmost eigenvalues.

If {\tt options\%err\_est$=$2}
the errors are estimated
based on the eigenvalue decrements history,
which produces an estimate for the average 
eigenvalue error reduction per iteration,
which in turn yields error estimates
for both eigenvalues and eigenvectors.
Unlike the residual estimates mentioned in this section, 
such `kinematic' error estimates are
not guaranteed to be upper bounds for the actual errors.
%(not even asymptotically, as with \Ref{KT} and \Ref{DK}).
However, the numerical tests have demonstrated
that kinematic error estimates 
are significantly more accurate,
i.e. closer to the actual error,
than the residual-based estimates. 
Furthermore, they straightforwardly
apply to the generalized case as well.

\subsubsection{Generalized problem}

In the case
of the generalized eigenvalue problem \Ref{evp.g}
solved by iterations with preconditioning,
all of the residual norms in the previous section must be replaced
with %\linebreak 
$\|\cdot\|_{B^{-1}}$-norm of the residual
%$A x_j^i - \lambda_j^i B x_j^i$
$r_j = A \tilde x_j - \tilde\lambda_j B \tilde x_j$
($\|r_j\|_{B^{-1}}^2 = r_j^* B^{-1} r_j$)
or its upper estimate, e.g. 
$\beta_1^{-1/2}\|\cdot\|$,
where $\beta_1$ is the smallest eigenvalue of $B$.
Hence, if $\beta_1$ is known, then
the error tolerances for eigenvalues and eigenvectors
must be multiplied by $\beta_1$ and $\sqrt{\beta_1}$
respectively. If no estimate for $\|\cdot\|_{B^{-1}}$-norm
is available, then the use of
non-zero residual tolerances and
{\tt options\%err\_est$=$1}
is not recommended.
In the case of problems \Ref{evp.g} solved by 
iterations with shift-and-invert
and the problem \Ref{evp.b},
the residuals are computed as
%the norm
%$\| T B x_j^i - \lambda_j^i x_j^i\|_B$
$r_j = T B \tilde x_j - \tilde \lambda_j \tilde x_j$
%is used, 
where
$T = (A - \sigma B)^{-1}$ for \Ref{evp.g} and
$T = (B - \sigma A)^{-1}$ for \Ref{evp.b},
and $B$-norms of $r_j$ are used, so that
Lehmann matrix becomes
$\hat A = \tilde\Lambda_k - S_k^T B\ S_k$.
\if 0
Note that the residual estimates 
may considerably overestimate the actual error of direct iterations
because  of the use of the Euclidean norm of the residual,
which is too strong a norm for it
when $A$ is the discretization of a differential operator.
\fi

\thebibliography{1}

\bibitem{report}
E.~E.~Ovtchinnikov and J.~Reid.
A preconditioned block conjugate gradient
algorithm for computing extreme eigenpairs
of symmetric and Hermitian problems.
\report, 2010.

\bibitem{jcpg1}
E.~E.~Ovtchinnikov,
{\em Jacobi correction equation, line search and
conjugate gradients in Hermitian eigenvalue computation I:
Computing an extreme eigenvalue},
SIAM J. Numer. Anal., {\bf 46}:2567--2592, 2008.

\bibitem{jcpg2}
E.~E.~Ovtchinnikov,
{\em Jacobi correction equation, line search and
conjugate gradients in Hermitian eigenvalue computation II:
Computing several extreme eigenvalues},
SIAM J. Numer. Anal., {\bf 46}:2593--2619, 2008.

\section{Examples}

\subsection{Preconditioning example}
\label{sec:ex.prec}

The following code 
computes the 5 leftmost eigenpairs of 
the matrix $A$ of order 100 that approximates 
the two-dimensional Laplacian operator
on a 20-by-20 grid.
One forward and one backward Gauss-Seidel update
are used for preconditioning,
which halves the number of iterations
compared with solving the same problem without preconditioning.
The module {\tt laplace2d} (\texttt{examples/Fortran/ssmfe/laplace2d.f90})
supplies the subroutine {\tt apply\_laplacian()}
that multiplies a block of vectors by $A$,
and the subroutine 
{\tt apply\_gauss\_seidel\_step()}
that computes $y = T x$ for a block of vectors $x$
by applying one forward and one backward update
of the Gauss-Seidel method to the system $A y = x$.
\verbatiminput{examples/Fortran/ssmfe/precond_expert.f90}
This code produces the following output:
\begin{verbatim}
  6 eigenpairs converged
 lambda( 1) = 4.4676695E-02
 lambda( 2) = 1.1119274E-01
 lambda( 3) = 1.1119274E-01
 lambda( 4) = 1.7770878E-01
 lambda( 5) = 2.2040061E-01
 lambda( 6) = 2.2040061E-01
\end{verbatim}

Note that the code computed one extra eigenpair
because of the insufficient gap between the 5th and 6th
eigenvalues.
